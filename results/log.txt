Starting Point:
1. Train_seq [4], approximate straight line after 40 epoch
2. Train_seq [10], approximate trajectory after 80 epoch
3. Train_seq [4, 10]. Test_seq [4, 10], trash results. Does not generalise well. 
4. Tran_Seq [1, 2, 4, 10], Test_seq [1, 2, 4, 10], increased regressor model capacity, 13 epochs, does not fit well
5. Train_seq [4, 10], Test_seq [4 10], increased regressor model capacity, 100 epochs, final tanh activation layer, intermediate tanh gives nan, changed to softplu. Rubbish training after 40 epochs.
6. train_seq [1], test_seq [1]. same setup. Does not work well
7. train_seq [1], test_seq [1], decrease warmup lr, increase fine tune lr. 

Possible experimentations from here
- GRU, LSTM
- Neural CDE
- Initialise hc + Repeat use of hc after each sequence
- Train on a smaller sequence
- Verify Network Construction by restructuring training script
- Absolute Tolerance (atol),  Relative Tolerance (rtol) 
https://uk.mathworks.com/help/matlab/math/troubleshoot-common-ode-problems.html
- Evaluation time stamps length

8. Repeat use of hc, train_seq [4], val_seq [4], 100 epoch does not give good rotation results
9. Repeat use of hc, 400 epoch, does not seem good. use non-overlapping sequences

* Pose error calculation uses relative pose, however pose regressor regress to absolute pose. Need to feed in the differential?

10. Use hidden states differential with stacked regressor, Revert back to overlapping sequences
11. Use hidden state differential by deducting hidden states, great improvement in training
12. Reduced atol, rtol to 10^-9 and 10^-6
13. Restore stepped lr and ep. Actually produced worse results. Should fine tune lr
14. train_seq [01 02 04], val_seq [01 02 04 10]
15. train_seq [04 10], val_seq [04, 10]. 04 seems not bad, but 10 goes wrong direction. should try to train 10 alone
16. train_seq [10], val_seq [10], does seems to resembles the path, but takes a long time. 

* Multiple Configurations For RNN input
1. output, new_hidden_states = self.rnn(fused_features.squeeze(1), new_hidden_states)
2. regress on output / regress on new_hidden_states/ regress on new_hidden_states + old_hidden_states

17. train_seq [10], val_seq [10], use rnn + stacked hidden states. Slow convergence, curly lines
18. train_seq [10], val_seq [10], use rnn + deducted hidden states. Straight lines like in 11. 
19. train_seq [10], val_seq [10], use gru + deducted hidden states. Converges even slower. Changed back to rnn
20. train_seq [4 10], val_seq [04 10], use rnn, 04 converges but sway, 10 froms a relatively similar shape but does not fit entirely

* Switched to regressing absolute pose from the first frame, then deduct the absolute pose from the previous prediction. 
21. train_seq [04], val_seq [04], use rnn, deduct predicted absoute pose from a frame before. 

* Maybe also test capping explotion or diminishing hidden states?
22. train_seq [10], val_seq [10], tried on path 10
23. train_seq [4, 10], val_seq [04, 10], it trains but not that good.
24. train_seq [4, 10], val_seq [04, 10], tried a few epochs for gru cell instead of rnncell, asbolutely garbage

* Forgot to initialise rnncell and grucell. Try again after ensuring correct initialisation. Can write in report about the different initialisation
* Forgot to add reduction = sum for loss calculation. 
25. train_seq [4, 10], val_seq [04, 10], added initialisation with gru, garbage training
26. train_seq [4, 10], val_seq [04, 10], added initialisation with rnn, normal rnn training
27. train_seq [01 02 04 05 10], val_seq [01 02 04 05 10], tried initialisation with rnn long training

* Revert back to differential with deduction
28. train_seq [01 02 04 05 10], val_seq [01 02 04 05 10], tried initialisation with rnn long training 

* Added data shuffling, Maybe that improves generalisation? 
29. train_seq [04 05 07 10], val_seq [04 05 07 10], data shuffling, tried initialisation with rnn long training SHUFFLING IS THE MAIN ISSUE!!! 

Why shuffling is important?
Prevent Overfitting: Without shuffling, the model might learn the order of the sequences rather than the underlying patterns. If the dataset contains sequences in a specific order (e.g., easy to hard, or by different driving conditions), the model could pick up on this sequence and overfit to that pattern. 
Balance Dataset: If the data is not shuffled, and there are imbalances in the dataset (e.g., certain types of sequences are clustered together), the model might learn to perform well on the overrepresented type at the expense of others. Shuffling helps to ensure that each mini-batch is more representative of the overall distribution of the data.
Stochastic Gradient Descent Efficiency: Models are often trained using stochastic gradient descent (SGD) or its variants. These algorithms assume that each mini-batch is an unbiased estimate of the overall gradient. 

* Now try to regress absolute pose then find the relative difference from last frame.
30. train_seq [04 05 07 10], val_seq [04 05 07 10], does not work well, error does not converge.

31. train_seq [04 05 07 10], val_seq [04 05 07 10], test using gru instead of rnn, gru fail to generalise and performed badly. 

* Introduced Dropout layer and see if it makes training better.
32. train_seq [04 05 07 10], val_seq [04 05 07 10], gru still performs badly
33. train_seq [04 05 07 10], val_seq [04 05 07 10], rnn to see if drop out layer makes it better. Does not make it better. Seems to trains poorly
34. train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10]. Perform actual training.

* Removed Dropout layer. Made it worse. Seems like overfitting did not happen, more like underfitting
36. train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10]. Perform actual training rnn no dropout training well at first, but then suddenly collapsed into circles after 11 epochs. Maybe need to reshuffle data after every single epoch.

* Added reshuffling at the start of each epoch
37. train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10]. Converges to circle at epoch 11, then loss comes back down and converge nicely. Best translation loss ~t_rel: 21.0623, r_rel: 4.2701
38. tested loading saved model. It works.

* Debugging Image Loading and Data sources
39. 

40. train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10]. Implemented Gradient Clipping at value 5. Seems like gradient clipping is effective at lowering the loss spike, but spike still occurs.

41. train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10], gradient clip=1

42. train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10], changed learning rate from 5e-4, 1e-4, 5e-5 to 5e-4, 5e-5, 1e-6

43. Tested run and continue experiments logging. Also run from epoch 6. every single time epoch 7 explodes. 
44. train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10], changed lr [5e-4, 5e-5, 1e-6] to lr [1e-4, 1e-5, 1e-6] performed worse with lower training rate. Try higher
45. train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10] new lr [1e-3, 1e-4, 1e-5] 


Things to test from here: 
1. Neural CDE
2. LTC Networks
3. Implement Dropout mechanism
4. Gradient Accumulation
5. Learning Rate decay/ lower learning rate
6. Linear Probing

46. train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10] restore restore lr [1e-4, 1e-5, 1e-6] Froze Encoder train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10] restore restore lr [1e-4, 1e-5, 1e-6] Unfroze Encoder at 50 epochs. Seems like the finetuning of encoder is not the bottleneck. 

Suspecting bottleneck might be in the NeuralODE network. Try increase the number of layers and keep frozen encoder

Also changed loggin formats
47. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10] increase ode_layers to 5. non frozen encoder. Does not outperform 3 layers.

Added data dropout functionality
48. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10] increase ode_layers to 5. non frozen encoder same config but data_dropout 0.1, eval_data_dropout 0.1. Dropout does affect training. Ode_layers does not help a lot. 

Maybe try using soft/hard fusion would work better. 
49. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10] ode_layers 5. frozen encoder for space, soft cat. 
50. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10] ode_layers 5. frozen encoder for space, hard cat.

soft/hard cat are better than cat, but soft> hard

50. train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10], run with gru to see if its better. 
51. train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10], run with SGD optimiser with GRU
GRU still rubbish 

52. train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10], run with SGD optimiser with rnn. SGD is rubbish
53. train_seq [00 02 08 09], val_seq [00 01 02 04 05 06 07 08 09 10], run with SGD optimiser with rnn, old lr [5e-4, 5e-5 1e-6] new lr [1e-4, 1e-5, 1e-6]
Both does not work well. SGD Optimiser is not as good as Adam. Revert back to Adam

Try new architecture levels. Maybe increase ODE Hidden?
54. Increased ode_hidden to 1024, lr [1e-3, 1e-4, 1e-5]
55. same config as above, ode_hidden=1024 batch_size=26
57.  lower ode_hidden_layers to 2, ode_hidden = 512 batch_size=12

Between 55 and 57, 57 does not converge but 55 performs well. Could be due to the batch size difference/ ODE layer is the bottleneck.
Can try: 1. 4 ode layers vs 2 ode layers + Gradient Accumulation. 

58. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10] batch size 12 4 ode layers. If it performs better then 57 then it is ode that's the bottleneck runs better but not great
59. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10] batch size 48 = 12*4 gradient accumulation, 2 ode layers.  runs ok but not great
60. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10] batch size 64 = 32 * 2 gradient accumulation 4 ode layers, hard fusion, froze encoder


Test using multiple RNN layers. Maybe it is the bottleneck. Try it with regressing output like in Visual Selective VIO.  
61. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10] 2 layers of rnn. regress directly on output. 
62. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10] 2 layers of rnn. regress on difference of initial and new states
63. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10] 4 layer ode 3 layers of rnn. regress on output directly.

65. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10] 0.1 dropout.
66. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10] 0.2 dropout.

Decided to change to Visual Selective VIO dataset. 
67. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10] 0 dropout rmse is very low
68. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10] 0.1 dropout rmse is worse, but not that much
69. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10] 0.2 dropout rmse is worse, but not that much

Repeated 70 training

70. train_seq [00 01 02 04 08 09], val_seq [06], test_seq[05 07 10] 0 dropout
71. train_seq [00 01 02 04 08 09], val_seq [06], test_seq[05 07 10] 0.1 dropout
72. train_seq [00 01 02 04 08 09], val_seq [06], test_seq[05 07 10] 0.2 dropout

73. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10], 0 dropout, best performing so far with around 7% error. 
74. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10], 0.1 dropout
75. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10], 0.2 dropout

New IMU Structure
76. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10], 0 dropout, try new imu structure and see if it improves training. Nope, it does not. Might need to revert to the Visual Selective VIO. 

76. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10] unfrozen encoder, 0.5 dropout, same config as other 
77. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10] unfrozen encoder, 0.7 dropout, same config as other

Reverted IMU
79. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10] 0.0 dropout. 100x lr for regressor. 

------------------------------------------------------------------------------------------------------
70. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10]. 48 batch size. 12*4 gradient accumulation. 4 layers rnn, 4 layers ode. regresson output directly.  Exploded in loss. 
71. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10]. 48 batch size. 12*4 gradient accumulation. 4 layers rnn, 4 layers ode. regresson output directly, lower learning rate [5e-5, 5e-6, 5e-7]
72. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10]. 48 batch size. Even lower lr [5e-5, 5e-6, 5e-7]
73. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10]. 48 batch size. 5rnn, 3 ode [1e-5, 1e-6, 1e-7]

it seems that lower lr actually performed worse. Increase initial lr
74. train_seq [00 02 08 09], val_seq [01 04 05 06 07 10] 80 batch size, 20*4, 3 layers rnn, 2 layers ode [1e-3, 1e-4, 1e-5]
------------------------------------------------------------------------------------------------------


100. train_seq [04 10], val_seq [04 10] use dopri5, relu, lr [1e-4, 1e-5, 1e-6] converges to around 0.5 loss at epoch 7
101. train_seq [04 10], val_seq [04 10] dopri5, relu, lr [1e-3, 1e-4, 1e-5] converges at around 5 loss at epoch 7
102. train_seq [04 10], val_seq [04 10] dopri5, leaky_relu, lr [1e-3, 1e-4, 1e-5]
try relu, prediction fluctuates, around 5-10 loss
103. train_seq [04 10], val_seq [04 10] dopri5, tanh, lr [1e-3, 1e-4, 1e-5]
All performed similarly bad. Does not converge quickly. 
104. train_seq [04 10], val_seq [04 10] dopri5, softplus and do not diff during output

Ideas to try out:
1. Increasing evaluation points by 10 times
2. Lower tolerance for rel and abs
3. Adjust CDE architecture (Diff lr for layers)
4. Choices of Solvers
5. Different Lr

105. train_seq [04 10], val_seq [04 10] dopri5, softplus lr [1e-1, 1e-2, 1e-3] super huge loss and exploded. 
This shows that a lower lr is better
106. train_seq [04 10], val_seq [04 10] dopri5, softplus lr [1e-7, 1e-8, 1e-9] high loss at first and slowly converges. Indicate should increase learning rate. 
107. train_seq [04 10], val_seq [04 10] dopri5, softplus lr [1e-6, 1e-7, 1e-8] still high loss at first and slowly converges. Indicate should increase learning rate. 
108. train_seq [04 10], val_seq [04 10] dopri5, softplus lr [1e-5, 1e-6, 1e-7] converges quickly from 600 to around 100
109. train_seq [04 10], val_seq [04 10] dopri5, softplus lr [5e-5, 5e-6, 5e-7] converges quickly from 600 to around 100

Use different solvers. 
110. train_seq [04 10], val_seq [04 10], midpoint solver
111. train_seq [04 10], val_seq [04 10], implicit_adams
112. train_seq [04 10], val_seq [04 10], explicit_adams
113. train_seq [04 10], val_seq [04 10], euler
114. train_seq [04 10], val_seq [04 10], rk4
115. train_seq [04 10], val_seq [04 10], evaluate 100 times. rk4
116. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10], evaluate 100 times. explicit_adams clip 0.5
117. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10], evaluate 100 times. explicit_adams clip 0.5 
118. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10], evaluate 100 times. clip 5, euler, no reduction

Implemented two things. 1. Stacked Neural CDE. 2. Regression layer 30xlr

119. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10] So far fastest to convergence. 

Try 100x lr for regression layer. 
120. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10] So far fastest to convergence. Still shitty initial loss. 

Try removing the initial layer but initialise as 0. Correctly concatenating multiple layers of hidden states. 
*** Break through ***
121. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10] Very low initial loss, super promising. lr [1e-4, 1e-5, 1e-6], only thing is that the loss does not seem to converge too quickly. Maybe reduce learning rate?
121. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10] lr [1e-5, 1e-6, 1e-7]
Turns out the loss converges at a very high point. Bad Predictions. 

Changed back the architecture into using linear and the first interpolation point with initial linear layer.
Also reverted back to consistent lr. 
123. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10]. The initial loss performs poorly. It shows that the initial state has a large effect on the initial performance. 

Difference in initial state and start with 0
125. train_seq [00 01 02 04 06 08 09], val_seq [05 07 10], smooth circles. Not what I am looking for
126. changed to dopri5 solver



132. removed outermost tanh layer
134. Longer seq_len